
Vision SLAM은 여러 방향으로 발전해왔고, 가장 최근 Deep learing 기술을 응용한 Semantic SLAM이 주목을 받고 있다. 왜일까. 
로봇의 활용성이 주목받기 시작한 것은 인간이 활동하던 영역에서 인간이 자리를 비우기 시작하면서부터다. 서빙 직원을 대신해서 로봇이 음식을 
운반해줘야 했고, 노동자를 대신하기 위해 용접로봇 솔루션들이 소규모 사업장에도 등장하기 시작했으며, 운전자가 없어진 택시는 자율주행지능에 맡겨졌다. 
그렇게 로봇이 점점 인간의 삶을 파고들고 주변에 빈번히 등장할수록 사람들이 일상적으로 생활하는 공간에서 활동해야 하는 니즈가 증가하고, 
그런 공간에 대한 이해가 있다면 더 많은 일들을 할 수 있을것이라는 전망을 내놓았다. 

지금까지 상업화된 많은 로봇들은 제한된 공간에서 일련의 정해진 작업들만 수행해왔다. 공장환경 중 한 지점에서 설치된 로봇 매니퓰레이터와, 이송로봇들은 
점,선,면,색,거리와 같은 파편적인 정보들만으로도 충분히 주어진 업무를 수행할 수 있었다. 

어쨌거나. 

Vision SLAM을 할거야. 
DeepLearning을 추가해서 Object SLAM을 할거야. 
왜냐하면, 매우 넓은 면적을 돌아다니며 Map정보를 저장해도 정보량이 크지 않아야 서칭에 부담이 없고, 메모리도 아낄수 있으니까. 
Obejct SLAM은 공간인식을 통해 VLM 기술과 연결되면서 보다 더 많은 가능성을 줄 수 있어. 
자연어 지시만으로 로봇에게 새로운 지시를 주는 것이 가능해지지. 

이런것들을 하려면 어떤 요구조건이 있을까
일단 컬러 영상을 받을 수 있어야 해. 넓고, 선명하게. 해상도가 높을수록 좋겠지. 적어도 사람은 사람의 유효 시야각만큼의 정보를 핸들링할 수 있길 바랄거야. 
그런 고해상도 고용량 데이터를 빠르게 핸들링 할수있어야 해. 
그럼 이것들 사이의 Trade-off는 무엇일까? 

이 다음 이렇게 얻은 이미지로 deeplearning을 하면, 문제는 의자1과 의자2를 구분하지 못하는 현상이 있어. 
이걸 Object들 간의 network로 연결하면 패턴인식의 문제로 전환할 수 있지 않을까?

그럼 패턴인식과 그래프노드를 연결하면 LiDAR슬램과 물려서 넓은 면적을 좀 더 수월하게 탐색할 수 있지 않을까? 

LiDAR Slam과 Vision SLAM으로 생성된 Map에 Objec는 어떻게 위치시킬까? 

Object들간의 Dept는 어떻게 추정할 수 있을까? 

Object들을 Fully Segmentation할건가? 아니면 Centerpoint만 잡아낼건가? 

Dynamic Object와 Static Object를 구분할건가? 구분해서 지도에 저장할건가? 
